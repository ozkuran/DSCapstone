twitterData <- '/data/final/en_US/en_US.twitter.txt'
blogData <- '/final/en_US/en_US.blogs.txt'
newsData <- '/final/en_US/en_US.news.txt'
twitter <- readLines(twitterData)
blogs <- readLines(blogData)
news <- readLines(newsData)
length(twitter)
length(news)
length(blogs)
twitter_max <- max(nchar(twitter))
news_max <- max(nchar(news))
blog_max <- max(nchar(blogs))
longestLine <- function(dataset) {
for line in 1:length(1:dataset) {
nchar
}
}
twitterData <- '/data/final/en_US/en_US.twitter.txt'
blogData <- '/final/en_US/en_US.blogs.txt'
newsData <- '/final/en_US/en_US.news.txt'
twitter <- readLines(twitterData)
blogs <- readLines(blogData)
news <- readLines(newsData)
length(twitter)
length(news)
length(blogs)
twitter_max <- max(nchar(twitter))
news_max <- max(nchar(news))
blog_max <- max(nchar(blogs))
longestLine <- function(dataset) {
for line in 1:length(1:dataset) {
nchar
}
}
love_string <- 'love'
hate_string <- 'hate'
love <- twitter[which(grepl(love_string, twitter))]
hate <- twitter[which(grepl(hate_string, twitter))]
length(love)/length(hate)
twitterData <- '/final/en_US/en_US.twitter.txt'
blogData <- '/final/en_US/en_US.blogs.txt'
newsData <- '/final/en_US/en_US.news.txt'
twitter <- readLines(twitterData)
blogs <- readLines(blogData)
news <- readLines(newsData)
length(twitter)
length(news)
length(blogs)
twitter_max <- max(nchar(twitter))
news_max <- max(nchar(news))
blog_max <- max(nchar(blogs))
longestLine <- function(dataset) {
for line in 1:length(1:dataset) {
nchar
}
}
love_string <- 'love'
hate_string <- 'hate'
love <- twitter[which(grepl(love_string, twitter))]
hate <- twitter[which(grepl(hate_string, twitter))]
length(love)/length(hate)
twitterData <- './final/en_US/en_US.twitter.txt'
blogData <- './final/en_US/en_US.blogs.txt'
newsData <- './final/en_US/en_US.news.txt'
twitter <- readLines(twitterData)
blogs <- readLines(blogData)
news <- readLines(newsData)
length(twitter)
length(news)
length(blogs)
twitter_max <- max(nchar(twitter))
news_max <- max(nchar(news))
blog_max <- max(nchar(blogs))
longestLine <- function(dataset) {
for line in 1:length(1:dataset) {
nchar
}
}
love_string <- 'love'
hate_string <- 'hate'
love <- twitter[which(grepl(love_string, twitter))]
hate <- twitter[which(grepl(hate_string, twitter))]
length(love)/length(hate)
twitterData <- './final/en_US/en_US.twitter.txt'
blogData <- './final/en_US/en_US.blogs.txt'
newsData <- './final/en_US/en_US.news.txt'
twitter <- readLines(twitterData)
blogs <- readLines(blogData)
news <- readLines(newsData)
length(twitter)
length(news)
length(blogs)
twitterMax <- max(nchar(twitter))
newsMax <- max(nchar(news))
blogMax <- max(nchar(blogs))
love<- sum(grepl("love", twitter))
hate<- sum(grepl("hate", twitter))
love/hate
biostats<- grep("biostats", twitter)
twitterData[biostats]
twitterData <- './final/en_US/en_US.twitter.txt'
#blogData <- './final/en_US/en_US.blogs.txt'
#newsData <- './final/en_US/en_US.news.txt'
twitter <- readLines(twitterData)
#blogs <- readLines(blogData)
#news <- readLines(newsData)
length(twitter)
#length(news)
#length(blogs)
twitterMax <- max(nchar(twitter))
#newsMax <- max(nchar(news))
#blogMax <- max(nchar(blogs))
love<- sum(grepl("love", twitter))
hate<- sum(grepl("hate", twitter))
love/hate
biostats<- grep("biostats", twitter)
twitter[biostats]
sum(grepl("A computer once beat me at chess, but it was no match for me at kickboxing", twitter))
This report is about Natural Language Processing project which developed as Capstone for Johns Hopkins Data Science Specialization. The NLP project is developed on SwiftKey dataset which may be found at [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). File is included **German**, **English*, **Finnish** and **Russian**  blog, news and twitter datasets. I have used **English** dataset of those to develop a text prediction application.
install.packages("tm")
install.packages("RWeka")
install.packages("SnowballC")
library(stringi)    # Character String Processing Facilities (general data information, word count etc..)
library(ggplot2)    # An Implementation of the Grammar of Graphics (visualization of data)
library(NLP)        # NLP infrastructure
library(tm)         # Text Mining Package (text mining tools)
library(RWeka)      # R/Weka interface (machine learning algorithms)
twitterFile <- './final/en_US/en_US.twitter.txt'
blogFile <- './final/en_US/en_US.blogs.txt'
newsFile <- './final/en_US/en_US.news.txt'
twitterData <- readLines(twitterFile)
blogsData <- readLines(blogFile)
newsData <- readLines(newsFile)
twitterStats <- stri_stats_general(twitterData)
blogsStats <- stri_stats_general(blogsData)
newsStats <- stri_stats_general(newsData)
twitterWords <- stri_count_words(twitterData)
blogsWords <- stri_count_words(blogsData)
newsWords <- stri_count_words(newsData)
twitterStats
blogsStats
newsStats
summary(twitterWords)
summary(blogsWords)
summary(newsWords)
sum(twitterWords)
sum(blogsWords)
sum(newsWords)
qplot(twitterWords, binwidth = 2)  + xlim(0,50) + ylab("Count") + xlab("Word Count per Tweet")
qplot(blogsWords, binwidth = 2)  + xlim(0,250) + ylab("Count") + xlab("Word Count per Blog Posts")
qplot(newsWords, binwidth = 2)  + xlim(0,250) + ylab("Count") + xlab("Word Count per News Article")
set.seed(54321)
sampleDataTwitter <- sample(twitterData, 23600)
sampleDataBlogs <- sample(blogsData, 9000)
sampleDataNews <- sample(newsData, 800)
data.raw <- c(sampleDataTwitter, sampleDataBlogs, sampleDataNews)
data.badwords <- readLines("badwords.txt")
data.stopwords <- stopwords('english')
cleanData <- function (d)
{
clean <- Corpus(VectorSource(d))
clean <- tm_map(clean, content_transformer(tolower))
clean <- tm_map(clean, removePunctuation)
clean <- tm_map(clean, removeNumbers)
clean <- tm_map(clean, stripWhitespace)
clean <- tm_map(clean, stemDocument)
clean <- tm_map(clean, removeWords, c(data.stopwords,data.badwords))
return (clean)
}
data.clean <- cleanData(data.raw)
install.packages("rJava")
install.packages("rJava")
find.java <- function() {
for (root in c("HLM", "HCU")) for (key in c("Software\\JavaSoft\\Java Runtime Environment",
"Software\\JavaSoft\\Java Development Kit")) {
hive <- try(utils::readRegistry(key, root, 2),
silent = TRUE)
if (!inherits(hive, "try-error"))
return(hive)
}
hive
}
find.java()
Sys.setenv(JAVA_HOME='C:\Program Files (x86)\Java\jre1.8.0_25\')
Sys.setenv(JAVA_HOME="C:\Program Files (x86)\Java\jre1.8.0_25\")
Sys.setenv(JAVA_HOME="C:\\Program Files (x86)\\Java\\jre1.8.0_25")
library(rJAva)
library(rJava)
